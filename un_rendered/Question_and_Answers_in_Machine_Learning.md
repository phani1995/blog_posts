

**Q. Is Gradient a vector or scalar..?**

Ans. The gradient is a vector operation which operates on a scalar function to produce a vector whose magnitude is the maximum rate of change of the function at the point of the gradient and which is pointed in the direction of that maximum rate of change. In rectangular coordinates the gradient of function f(x,y,z) is: 

$$
\nabla f =[i\frac{\partial}{\partial x}+j\frac{\partial}{\partial y}+\frac{\partial}{\partial z}]f
$$

If S is a surface of constant value for the function f(x,y,z) then the gradient on the surface defines a vector which is normal to the surface.

**Q. Difference between a gradient and directional derivative..?**

Gradient is multidimensional rate of change of given function and Directional derivative is the projection of that Gradient


**Q. Difference between a gradient and derivative..?**

derivative is in 2-D, for multi-dimensions its gradient (partial derivative with respective each direction). Derivative is a scalar whereas gradient is vector.


**Q.what is Jacobian  Matrix ..?**

**Q.what is Hessian Matrix ..?**

**Q.what is Newtonâ€“Raphson method ..?**

**Q.what is Difference between gradient descent and stochastic gradient descent ..?**
Gradient descent to take one step pass through all training samples.   
Stochastic gradient descent pass through single sample of training samples to take a step.  
mini batch gradient descent pass through small portion in powers of two (32,64,128) of training samples to take a step.


