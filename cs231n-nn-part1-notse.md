
# Activation functions

**sigmoid** : sigmoid non-linearity squashes real numbers to range between [0,1]
**tanh** : tanh non-linearity squashes real number to range between [-1,1]
**RelU** : f(x) = max(0,x)
**Leaky RelU**:
**Maxout**: 

Disadvantages of Sigmoid :
* sigmoid saturate  and kill gradients
* sigmoid is non-zero centerd

metrics
**sizing neural network**

> as we increase the size and number of layers in a neural network the *capacity* of the network increases.

> Small neural network is prone to over fitting
